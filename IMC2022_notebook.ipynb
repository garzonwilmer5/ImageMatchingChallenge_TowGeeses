{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.014409,"end_time":"2022-04-22T17:27:27.894541","exception":false,"start_time":"2022-04-22T17:27:27.880132","status":"completed"},"tags":[]},"source":["<h1>Image Matching Challenge 2022</h1>\n","<h2>Team towGeeses</h2>\n"," <ul>\n","  <li>Wilmer David Garzón Cáceres</li>\n","  <li>Juan Sebastian Santcoloma Barrera</li>\n","</ul>\n","\n","<p> Solution of place 45/642 <b>(top 8%)</b></p>\n","\n","<h3>Explanation</h3>\n","<p>This solution combain use the models LoFTR [1] ans SuperGlue [2] to find key point matches in a pair of images,\n","each image is preprocesed with geometrical transformations in order to make an augmentation of the data. \n","The points coordinates finded by geometrical transformation must be returnned to original image coordinates.</p>\n","<p>The fundamental matrix is finded using the robust estimator MAGSAC++ [3] avalible in openCV.</p>\n","\n","<h3>References</h3>\n","[1] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, ‘LoFTR: Detector-Free Local Feature Matching with Transformers’, CVPR, 2021.\n","<br/>\n","[2] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, ‘SuperGlue: Learning Feature Matching with Graph Neural Networks’, CVPR, 2020.\n","<br/>\n","[3]\tD. Barath, J. Noskova, M. Ivashechkin, and J. Matas, ‘MAGSAC++, a fast, reliable and accurate robust estimator’. arXiv, 2019."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.017024,"end_time":"2022-04-22T17:28:29.85595","exception":false,"start_time":"2022-04-22T17:28:29.838926","status":"completed"},"tags":[]},"source":["# ***Import dependencies***"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-07T22:13:53.104977Z","iopub.status.busy":"2022-06-07T22:13:53.104384Z","iopub.status.idle":"2022-06-07T22:13:53.819229Z","shell.execute_reply":"2022-06-07T22:13:53.818406Z","shell.execute_reply.started":"2022-06-07T22:13:53.104934Z"},"papermill":{"duration":2.469375,"end_time":"2022-04-22T17:28:32.342741","exception":false,"start_time":"2022-04-22T17:28:29.873366","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import cv2\n","import csv\n","import torch\n","import matplotlib.pyplot as plt\n","from kornia_moons.feature import *\n","import kornia as K\n","import kornia.feature as KF\n","import gc\n","import time\n","from typing import Dict, List, Tuple, Callable, Optional\n","\n","\n","import sys\n","sys.path.append(\"../SuperGluePretrainedNetwork\")\n","from models.matching import Matching"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.01725,"end_time":"2022-04-22T17:28:32.378076","exception":false,"start_time":"2022-04-22T17:28:32.360826","status":"completed"},"tags":[]},"source":["# ***Model***"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-06-07T22:13:53.820899Z","iopub.status.busy":"2022-06-07T22:13:53.820541Z","iopub.status.idle":"2022-06-07T22:13:56.111265Z","shell.execute_reply":"2022-06-07T22:13:56.110463Z","shell.execute_reply.started":"2022-06-07T22:13:53.820862Z"},"papermill":{"duration":4.74373,"end_time":"2022-04-22T17:28:37.138754","exception":false,"start_time":"2022-04-22T17:28:32.395024","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#LoFTR model\n","matcher = KF.LoFTR(pretrained=None)\n","matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n","matcher = matcher.to(device).eval()\n","\n","#SuperGLUE model\n","config = {\n","    \"superpoint\": {\n","        \"nms_radius\": 4,\n","        \"keypoint_threshold\": 0.005,\n","        \"max_keypoints\": 1024\n","    },\n","    \"superglue\": {\n","        \"weights\": \"outdoor\",\n","        \"sinkhorn_iterations\": 20,\n","        \"match_threshold\": 0.2,\n","    }\n","}\n","matching = Matching(config).eval().to(device)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016801,"end_time":"2022-04-22T17:28:37.172981","exception":false,"start_time":"2022-04-22T17:28:37.15618","status":"completed"},"tags":[]},"source":["## *Utils*"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-06-07T22:18:07.180589Z","iopub.status.busy":"2022-06-07T22:18:07.180273Z","iopub.status.idle":"2022-06-07T22:18:07.212589Z","shell.execute_reply":"2022-06-07T22:18:07.211777Z","shell.execute_reply.started":"2022-06-07T22:18:07.180556Z"},"papermill":{"duration":0.035034,"end_time":"2022-04-22T17:28:37.22538","exception":false,"start_time":"2022-04-22T17:28:37.190346","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["src = '../image-matching-challenge-2022/'\n","\n","test_samples = []\n","with open(f'{src}/test.csv') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    for i, row in enumerate(reader):\n","        # Skip header.\n","        if i == 0:\n","            continue\n","        test_samples.extend([row])\n","\n","\n","def FlattenMatrix(M: np.ndarray, num_digits: Optional[int] = 8):\n","    '''Convenience function to write CSV files.'''\n","    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n","\n","def cv2_Torch(img: np.ndarray, device: torch.device)->torch.Tensor:\n","    img = K.image_to_tensor(img, False).float() /255.\n","    img = img.to(device)\n","    return img\n","\n","def preprocessing_image(fname: str, device: torch.device) -> (Dict[str,torch.Tensor], float):\n","    \"\"\"\n","     Scale, normalize and apply geometrical transformations to an image given its path\n","    \"\"\"\n","    img = cv2.imread(fname)\n","    scale = 840 / max(img.shape[0], img.shape[1]) \n","    w = int(img.shape[1] * scale)\n","    h = int(img.shape[0] * scale)\n","    img = cv2.resize(img, (w, h))\n","    img = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)\n","    # Fliped images\n","    imgV = cv2.flip(img, 0)\n","    imgH = cv2.flip(img, 1)\n","               \n","    images = {'base':img,\n","             'vFlipped': imgV,\n","             'hFlipped': imgH\n","             }   \n","    images = {k: cv2_Torch(v,device) for k, v in images.items()}\n","    \n","    return images, scale\n","                        \n","def reverseMirrorPoints(points: np.ndarray, length: List[int], axis: List[int]) -> np.ndarray:\n","    for l,a in zip(length,axis):\n","        points[:,a] = l - points[:,a] - 1 \n","    return points\n","\n","def reverseRotation(points: np.ndarray, imgShape: Tuple[int,int] ,clockWise: bool) -> np.ndarray:\n","    h,w = imgShape\n","    (cX, cY) = (w // 2, h // 2)\n","    M = (1 if clockWise else -1)*np.array([[0,1],[-1,0]])\n","    points = (points-[cX, cY]) @ M + [cY, cX]\n","    return points\n","                        \n","def getLoFTRMatches(\n","        matcher: KF.LoFTR,\n","        input_dict: dict, th: Optional[float] = 0.4,\n","        foo: Optional[Callable] = None,\n","        fooParams1: Optional[dict] = {},\n","        fooParams2: Optional[dict] = {}) -> (np.ndarray,np.ndarray):\n","\n","    with torch.no_grad():\n","        correspondences = matcher(input_dict)\n","                        \n","    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n","    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n","    select = correspondences['confidence'].cpu().numpy() > th\n","\n","    if foo:\n","        mkpts0 = foo(mkpts0[select,:],**fooParams1)\n","        mkpts1 = foo(mkpts1[select,:],**fooParams2)\n","    else:\n","        mkpts0 = mkpts0[select,:]\n","        mkpts1 = mkpts1[select,:]\n","\n","    return mkpts0, mkpts1\n","                        \n","def getSuperGLUEMatches(\n","        matcher: Matching,\n","        input_dict: dict,\n","        foo: Optional[Callable] = None,\n","        fooParams1: Optional[dict] = {},\n","        fooParams2: Optional[dict] = {}) -> (np.ndarray,np.ndarray):\n","    pred = matcher(input_dict)\n","    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n","    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n","    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n","\n","    valid = matches > -1\n","    mkpts0 = kpts1[valid]\n","    mkpts1 = kpts2[matches[valid]]\n","\n","    if foo:\n","        mkpts0 = foo(mkpts0,**fooParams1)\n","        mkpts1 = foo(mkpts1,**fooParams2)\n","\n","    return mkpts0, mkpts1\n","                        \n","def plotMatches(\n","        image_1: torch.Tensor,\n","        image_2: torch.Tensor,\n","        mkpts0: np.ndarray,\n","        mkpts1: np.ndarray,\n","        inliers: np.ndarray):\n","    draw_LAF_matches(\n","        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n","                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n","                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n","\n","        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n","                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n","                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n","        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n","        K.tensor_to_image(image_1),\n","        K.tensor_to_image(image_2),\n","        inliers,\n","        draw_dict={'inlier_color': (0.2, 1, 0.2),\n","                   'tentative_color': None, \n","                   'feature_color': (0.2, 0.5, 1), 'vertical': False})"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016772,"end_time":"2022-04-22T17:28:37.259611","exception":false,"start_time":"2022-04-22T17:28:37.242839","status":"completed"},"tags":[]},"source":["# ***Inference***"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-06-07T22:18:09.142562Z","iopub.status.busy":"2022-06-07T22:18:09.142279Z","iopub.status.idle":"2022-06-07T22:18:34.364793Z","shell.execute_reply":"2022-06-07T22:18:34.363794Z","shell.execute_reply.started":"2022-06-07T22:18:09.142532Z"},"papermill":{"duration":18.780795,"end_time":"2022-04-22T17:28:56.057457","exception":false,"start_time":"2022-04-22T17:28:37.276662","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["F_dict = {}\n","\n","for i, row in enumerate(test_samples):\n","    sample_id, batch_id, image_1_id, image_2_id = row\n","    # Load the images.\n","    st = time.time()\n","    images1, scale1 = preprocessing_image(f'{src}/test_images/{batch_id}/{image_1_id}.png', device)\n","    images2, scale2 = preprocessing_image(f'{src}/test_images/{batch_id}/{image_2_id}.png', device)\n","    \n","    input_dict = {key:{\"image0\": K.color.rgb_to_grayscale(images1[key]),\n","                       \"image1\": K.color.rgb_to_grayscale(images2[key])} for key in images1.keys()}\n","    # Get image shapes\n","    w1 = images1['base'].shape[-1]\n","    h1 = images1['base'].shape[-2]\n","    w2 = images2['base'].shape[-1]\n","    h2 = images2['base'].shape[-2]\n","    # Functions to invert geometrical transformations\n","    reverseFoo = {'base': {'foo': None},\n","                  'vFlipped': {'foo': reverseMirrorPoints,\n","                               'fooParams1': {'length':[h1],'axis':[1]},\n","                               'fooParams2': {'length':[h2],'axis':[1]}},\n","                  'hFlipped': {'foo': reverseMirrorPoints,\n","                               'fooParams1': {'length':[w1],'axis':[0]},\n","                               'fooParams2': {'length':[w2],'axis':[0]}},\n","                 }\n","    # Get maching pairs\n","    mkptsGlue = [getSuperGLUEMatches(matching, v, **reverseFoo[k]) for k,v in input_dict.items()]\n","    mkpts = [getLoFTRMatches(matcher, v,th=0.3, **reverseFoo[k]) for k,v in input_dict.items()]\n","    mkpts.extend(mkptsGlue)\n","    \n","    mkpts0 = np.empty((0,2))\n","    mkpts1 = np.empty((0,2))\n","    for points in mkpts:\n","        mkpts0 = np.concatenate((mkpts0, points[0]), axis=0)\n","        mkpts1 = np.concatenate((mkpts1, points[1]), axis=0)\n","    mkpts0 /= scale1\n","    mkpts1 /= scale2\n","    \n","    \n","    if len(mkpts0) > 7:  \n","        F, inliers = cv2.findFundamentalMat(\n","            mkpts0, \n","            mkpts1, \n","            cv2.USAC_MAGSAC,\n","            0.2,\n","            0.999999,\n","            180_000\n","        )\n","        plotMatches(images1['base'],\n","                    images2['base'],\n","                    mkpts0 * scale1,\n","                    mkpts1 * scale2,\n","                    inliers)\n","        \n","        assert F.shape == (3, 3), 'Malformed F?'\n","        F_dict[sample_id] = F\n","    else:\n","        F_dict[sample_id] = np.zeros((3, 3))\n","        continue\n","    gc.collect()\n","    nd = time.time()    \n","    if (i < 3):\n","        print(f\"Total key points pair: {len(inliers)}, Inliers pairs:{sum(inliers[0])}\")\n","        print(\"Running time: \", nd - st, \" s\")\n","        print(\"Fundamental Matrix:\", F)\n","\n","with open('submission.csv', 'w') as f:\n","    f.write('sample_id,fundamental_matrix\\n')\n","    for sample_id, F in F_dict.items():\n","        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
